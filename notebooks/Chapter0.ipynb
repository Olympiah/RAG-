{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmb_aw4G78aU"
      },
      "source": [
        "## Chapter 0: Setup\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/edu/blob/main/rag-advanced/notebooks/Chapter00.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "<!--- @wandbcode{rag-course-00} -->\n",
        "\n",
        "Let's install the required packages and check our setup for this course.\n",
        "\n",
        "###  Free Cohere API key\n",
        "\n",
        "Before you run this colab notebook, head over to this [link to redeem a free Cohere API key](https://docs.google.com/forms/d/e/1FAIpQLSc9x4nV8_nSQvJnaINO1j9NIa2IUbAJqrKeSllNNCCbMFmCxw/viewform?usp=sf_link).\n",
        "\n",
        "Alternatively if you have a Cohere API key feel free to proceed. :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j308J4II78ab",
        "outputId": "dcd992c2-cd79-49c3-d339-32da5d352d84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/982.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m501.8/982.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m982.3/982.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m310.7/310.7 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m388.4/388.4 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m311.0/311.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m488.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m565.6/565.6 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq weave cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swuqvto778ad"
      },
      "source": [
        "## 1. Setup Weave\n",
        "\n",
        "\n",
        "The code cell below will prompt you to put in a W&B API key. You can get your API key by heading over to https://wandb.ai/authorize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "9xPb2VVj78ad",
        "outputId": "e8ca0240-3a11-4b0c-f341-3c6fa5eb731a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
            "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
            "Please login to Weights & Biases (https://wandb.ai/) to continue:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 路路路路路路路路路路\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "WARNING:weave.trace.env:There are different credentials in the netrc file and the environment. Using the environment value.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged in as Weights & Biases user: olympiah.\n",
            "View Weave data at https://wandb.ai/olympiah-fellowshipai/rag_course/weave\n"
          ]
        }
      ],
      "source": [
        "# import weave\n",
        "import weave\n",
        "\n",
        "# initialize weave client\n",
        "weave_client = weave.init(\"rag_course\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhl1f-Bl78ae"
      },
      "source": [
        "## 2. Setup Cohere\n",
        "\n",
        "The code cell below will prompt you to put in a Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlWZM8Nm78ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ae69e1-e4e1-47b1-8c39-ae3b3437044b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your COHERE_API_KEY路路路路路路路路路路\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "\n",
        "import cohere\n",
        "\n",
        "cohere_client = cohere.ClientV2(\n",
        "    api_key=getpass.getpass(\"Please enter your COHERE_API_KEY\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-8SoGev78af"
      },
      "source": [
        "## A simple-turn chat with Cohere's command-r-plus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIdQ4zBs78ag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aee9d9d4-2af3-467a-d387-8418e44d4931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " https://wandb.ai/olympiah-fellowshipai/rag_course/r/call/01926bfa-638f-7913-8ff6-b523fd8a75e5\n"
          ]
        }
      ],
      "source": [
        "response = cohere_client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What is retrieval augmented generation (RAG)?\"}\n",
        "    ],\n",
        "    model=\"command-r-plus\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=2000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkFxJguO78ah"
      },
      "source": [
        "Let's head over to the weave URL to check out the generated response."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzCXq0zv-72z",
        "outputId": "c1a825dd-bcb1-4c2b-ee29-2fc15d01da20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id='4f0c843b-faf4-41b6-b96f-8e2d4f0b3cb1' finish_reason='COMPLETE' prompt=None message=AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text='Retrieval-Augmented Generation (RAG) is a natural language processing (NLP) technique that combines information retrieval (IR) with language generation. It aims to enhance the quality and factual accuracy of generated text by retrieving relevant information from an external source of text, such as a large corpus of documents or a structured knowledge base.\\n\\nHere\\'s a simplified breakdown of how RAG works:\\n\\n1. **Information Retrieval**: RAG starts by understanding the input, which could be a question, a prompt, or a partial piece of text. Based on this input, it performs information retrieval by searching through a pre-defined source of text, often called the \"retrieval corpus.\" This corpus can be a collection of documents, web pages, books, or any textual data relevant to the task at hand. The retrieval step aims to find passages or sentences from the corpus that are relevant to the input.\\n\\n2. **Evidence Aggregation**: The retrieved relevant passages are then processed to select the most useful and informative pieces of evidence. This step may involve techniques such as passage ranking, filtering, or summarization to distill the most pertinent information from the retrieved text.\\n\\n3. **Generation**: The selected evidence, along with the original input, is fed into a language generator, typically a neural network-based model like a Transformer. The generator uses the retrieved evidence as additional context to guide the generation process. This helps the model produce responses that are not only coherent and contextually appropriate but also factually grounded in the retrieved information.\\n\\n4. **Output**: The final output of the RAG model is a generated text that incorporates information from the retrieved evidence. This could be an answer to a question, a continuation of a story, a summary, or any other form of text generation, depending on the specific task and input.\\n\\nThe key advantage of RAG is that it reduces the reliance solely on the patterns and knowledge learned during the model\\'s training phase. By retrieving and utilizing external information, RAG can generate responses that are more accurate, informative, and up-to-date, even for topics that were not extensively covered in its training data.\\n\\nRAG has been applied to various NLP tasks, including question answering, dialogue generation, text completion, and summarization. It has shown promising results in improving the factual correctness of generated text, which is a significant challenge in language generation, especially for open-domain chatbots and virtual assistants.\\n\\nOverall, RAG combines the strengths of information retrieval and language generation to create more reliable and informative text-generating systems.')], citations=None) usage=Usage(billed_units=UsageBilledUnits(input_tokens=9.0, output_tokens=519.0, search_units=None, classifications=None), tokens=UsageTokens(input_tokens=207.0, output_tokens=519.0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pwpJ0kkt-8yC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}